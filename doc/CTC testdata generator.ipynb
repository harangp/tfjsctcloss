{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC loss Python examples\n",
    "Test python script to generate various examples for CTC loss and gradien calculation\n",
    "Let's start with importing tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define two vaiables: one with the hypothetical truth, and the other, the logist (basically this is what comes out of your model). Please note, that python's ctc calculation requires the labels to be provided as a list of numbers that correspond to the embedding's index, whereas the TFJS implementation does not allow you to do that, since the labels' shape must be tha same as the logits' shape. So the thing in TFJS, you would have a tensor like this:\n",
    "``` JS\n",
    "[[\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "]]\n",
    "```\n",
    "whereas in Python, you would have this:\n",
    "``` python\n",
    "[[0, 1, 2, 3]]\n",
    "```\n",
    "Also, take notice, that the default structure of the logits is a Tensor of shape [frames, batch_size, num_labels]. If logits_time_major == False, shape is [batch_size, frames, num_labels]. The JS implementeation goes with the latter structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[0, 1, 0, 1, 3]]\n",
    "#\t[batch_size, frames, num_labels].\n",
    "logits = [[\n",
    "    [1.0, 0.0, 0.0, 0.0], \n",
    "    [0.0, 0.0, 0.0, 1.0], \n",
    "    [1.0, 0.0, 0.0, 0.0], \n",
    "    [0.0, 0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "]]\n",
    "labels_length = 4\n",
    "logits_length = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert everything to tensors, because that's what TF likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "logits_tensor = tf.Variable(tf.convert_to_tensor(logits, dtype=tf.float32))\n",
    "labels_length_tensor = tf.convert_to_tensor([labels_length], dtype=tf.int32)\n",
    "logits_length_tensor = tf.convert_to_tensor([logits_length], dtype=tf.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around with TF features used by the python implementation: the `tf.nn.ctc_loss` performs a log_softmax on the input to normalize the inputs. This approach comes from the implementation, since it generates the gradients relative to the *unnormalized* inputs. So, the softmax must be inside of the loss calculation. \n",
    "For future reference: log_softmax is just softmax, but then the logarithm is calculated elementwise.\n",
    "The alg. frequently calculates with infinities. So I just put in there some of the measures they are using here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/ctc_ops.py#L880-L1033 The main thing is, if you see losses in the 700's region, it is practically infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(logits): [[[0.47536692 0.17487772 0.17487772 0.17487772]\n",
      "  [0.17487772 0.17487772 0.17487772 0.47536692]\n",
      "  [0.47536692 0.17487772 0.17487772 0.17487772]\n",
      "  [0.17487772 0.17487772 0.17487772 0.47536692]\n",
      "  [0.17487772 0.17487772 0.17487772 0.47536692]]]\n",
      "log(softmax(logits)): [[[-0.7436683 -1.7436683 -1.7436683 -1.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]\n",
      "  [-0.7436683 -1.7436683 -1.7436683 -1.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]]]\n",
      "log_softmax logits: [[[-0.7436683 -1.7436683 -1.7436683 -1.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]\n",
      "  [-0.7436683 -1.7436683 -1.7436683 -1.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]\n",
      "  [-1.7436683 -1.7436683 -1.7436683 -0.7436683]]]\n",
      "log(zero): tf.Tensor(-inf, shape=(), dtype=float32)\n",
      "casted logZero for TPU: tf.Tensor(-706.8936, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"softmax(logits):\", tf.nn.softmax(logits_tensor).numpy())\n",
    "print(\"log(softmax(logits)):\", tf.math.log(tf.nn.softmax(logits_tensor)).numpy())\n",
    "print(\"log_softmax logits:\", tf.nn.log_softmax(logits_tensor).numpy())\n",
    "print(\"log(zero):\", math_ops.log(0.0))\n",
    "print(\"casted logZero for TPU:\", math_ops.cast(math_ops.log(math_ops.cast(0, dtypes.float64) + 1e-307), dtypes.float32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the CTC loss, and the gradient. Your other implementation must be able to generate these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC loss: [4.448741]\n",
      "CTC gradients:  [[[-0.51064587  0.17487772  0.17487772  0.16089036]\n",
      "  [ 0.12286875 -0.6697599   0.17487772  0.37201348]\n",
      "  [-0.29322746 -0.01850624  0.17487772  0.13685611]\n",
      "  [-0.15988137 -0.20941935  0.17487772  0.1944232 ]\n",
      "  [ 0.17487772 -0.5441786   0.17487772  0.19442326]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(logits_tensor)\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels_tensor, \n",
    "        logits_tensor, \n",
    "        labels_length_tensor, \n",
    "        logits_length_tensor, \n",
    "        logits_time_major=False,\n",
    "        blank_index=-1,\n",
    "        name=\"test\"\n",
    "    )\n",
    "    grads = tape.gradient(loss, logits_tensor)\n",
    "\n",
    "    print(\"CTC loss:\", loss.numpy())\n",
    "    print(\"CTC gradients: \", grads.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just another test run on a random uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_label [[1 2 4 3]]\n",
      "test_logits [[[0.84265196 0.7786586  0.4704218  0.3375883 ]]\n",
      "\n",
      " [[0.00791407 0.37151897 0.74040926 0.21611357]]\n",
      "\n",
      " [[0.4604901  0.52297366 0.22432959 0.20178378]]\n",
      "\n",
      " [[0.5362582  0.7950176  0.8498398  0.67813313]]]\n"
     ]
    }
   ],
   "source": [
    "test_label = tf.random.uniform(\n",
    "    [1, labels_length],\n",
    "    minval=1, \n",
    "    maxval=logits_length, \n",
    "    dtype=tf.int64\n",
    ")\n",
    "print('test_label', test_label.numpy())\n",
    "# [num_frames, batch_size, num_labels]\n",
    "test_logits = tf.random.uniform([4, 1, 4])\n",
    "print('test_logits', test_logits.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d7d8f1925d1087d0b0bc8cd3cb4748a17a2c7d9ad3b2d3aeed6e770cd078465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
